INSTALL THIS ---> pip install music21 mido numpy tensorflow keras pretty_midi
FOIL
AI_Music_Generator/
│
├── data/                  # MIDI dataset folder
├── utils.py               # Preprocessing functions
├── lstm_model.py          # RNN/LSTM model
├── transformer_model.py   # Transformer model
├── train.py               # Training loop
├── generate.py            # Generation script
└── midi_utils.py          # MIDI input/output




# utils.py
from music21 import converter, instrument, note, chord
import glob

def extract_notes_from_midi(midi_folder):
    notes = []
    for file in glob.glob(f"{midi_folder}/*.mid"):
        midi = converter.parse(file)
        parts = instrument.partitionByInstrument(midi)
        notes_to_parse = parts.parts[0].recurse() if parts else midi.flat.notes
        for element in notes_to_parse:
            if isinstance(element, note.Note):
                notes.append(str(element.pitch))
            elif isinstance(element, chord.Chord):
                notes.append('.'.join(str(n) for n in element.normalOrder))
    return notes


# lstm_model.py
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, Activation

def create_lstm_model(input_shape, n_vocab):
    model = Sequential()
    model.add(LSTM(512, input_shape=input_shape, return_sequences=True))
    model.add(Dropout(0.3))
    model.add(LSTM(512))
    model.add(Dense(256))
    model.add(Dropout(0.3))
    model.add(Dense(n_vocab))
    model.add(Activation('softmax'))
    model.compile(loss='categorical_crossentropy', optimizer='adam')
    return model


# transformer_model.py
from tensorflow.keras import layers, models

def transformer_model(seq_len, vocab_size, d_model=128, num_heads=4, ff_dim=512, num_layers=4):
    inputs = layers.Input(shape=(seq_len,))
    x = layers.Embedding(vocab_size, d_model)(inputs)

    for _ in range(num_layers):
        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)
        x = layers.Add()([x, attn_output])
        x = layers.LayerNormalization()(x)
        ff_output = layers.Dense(ff_dim, activation='relu')(x)
        ff_output = layers.Dense(d_model)(ff_output)
        x = layers.Add()([x, ff_output])
        x = layers.LayerNormalization()(x)

    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(256, activation='relu')(x)
    x = layers.Dense(vocab_size, activation='softmax')(x)

    model = models.Model(inputs, x)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
    return model


# train.py
import numpy as np
from keras.utils import to_categorical
from utils import extract_notes_from_midi
from lstm_model import create_lstm_model

notes = extract_notes_from_midi("data")
unique_notes = sorted(set(notes))
note_to_int = dict((note, number) for number, note in enumerate(unique_notes))

seq_length = 100
network_input = []
network_output = []

for i in range(len(notes) - seq_length):
    sequence_in = notes[i:i + seq_length]
    sequence_out = notes[i + seq_length]
    network_input.append([note_to_int[note] for note in sequence_in])
    network_output.append(note_to_int[sequence_out])

n_patterns = len(network_input)
network_input = np.reshape(network_input, (n_patterns, seq_length, 1))
network_input = network_input / float(len(unique_notes))
network_output = to_categorical(network_output)

model = create_lstm_model((network_input.shape[1], 1), len(unique_notes))
model.fit(network_input, network_output, epochs=100, batch_size=64)
model.save('lstm_music_model.h5')


# generate.py
import numpy as np
from keras.models import load_model
from midi_utils import create_midi
from utils import extract_notes_from_midi

notes = extract_notes_from_midi("data")
unique_notes = sorted(set(notes))
note_to_int = dict((note, number) for number, note in enumerate(unique_notes))
int_to_note = dict((number, note) for number, note in enumerate(unique_notes))

sequence_length = 100
start = np.random.randint(0, len(notes)-sequence_length-1)
pattern = [note_to_int[n] for n in notes[start:start + sequence_length]]

model = load_model('lstm_music_model.h5')
generated = []

for _ in range(500):
    input_seq = np.reshape(pattern, (1, len(pattern), 1))
    input_seq = input_seq / float(len(unique_notes))
    prediction = model.predict(input_seq, verbose=0)
    index = np.argmax(prediction)
    result = int_to_note[index]
    generated.append(result)
    pattern.append(index)
    pattern = pattern[1:]

create_midi(generated, "output.mid")


# midi_utils.py
from music21 import stream, note, chord

def create_midi(prediction_output, filename="output.mid"):
    output_notes = []

    for pattern in prediction_output:
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            chord_notes = [note.Note(int(n)) for n in notes_in_chord]
            new_chord = chord.Chord(chord_notes)
            new_chord.duration.quarterLength = 0.5
            output_notes.append(new_chord)
        else:
            new_note = note.Note(pattern)
            new_note.duration.quarterLength = 0.5
            output_notes.append(new_note)

    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp=filename)
